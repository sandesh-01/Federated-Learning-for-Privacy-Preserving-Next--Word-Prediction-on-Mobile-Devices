{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JL3ZPJPgC4wC"
   },
   "source": [
    "## Word Prediction in a Mobile Device Federation\n",
    "\n",
    "- [x] Get the data\n",
    "- [x] Data describe \n",
    "- [x] Preprocessing \n",
    "- [x] Data exploration\n",
    "- [x] Create tensors\n",
    "> NOTE: From this point on, the data is in the same pattern as the tff simulation module\n",
    "- [x] Data federated understanding\n",
    "- [x] Load the federated data\n",
    "> NOTE: now on the next steps will be done on a federation client\n",
    "- [x] Preprocess the data \n",
    "- [ ] load pre-trained model each client\n",
    "- [ ] Compile the model and test on the preprocessed data\n",
    "- [ ] Fine-tune the model with Federated Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "SpueB6zADYgE",
    "outputId": "2b70a15a-f372-4a95-e16f-40aa4dc5e5d0"
   },
   "outputs": [],
   "source": [
    "# data analysis and data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from plotly.offline import plot, iplot\n",
    "import plotly.graph_objs as go # create graphics\n",
    "import plotly.offline as py\n",
    "import plotly\n",
    "\n",
    "import cufflinks as cf # conect ploty and pandas\n",
    "import plotly.io as pio\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "# ml and dl\n",
    "import tensorflow_federated as tff\n",
    "import tensorflow as tf\n",
    "\n",
    "## Natural Processing Language\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# other\n",
    "from imp import reload\n",
    "\n",
    "import platform\n",
    "import warnings\n",
    "import pathlib\n",
    "import pprint\n",
    "import string\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Work Directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_work_directory(end_directory: str='notebooks'):\n",
    "    # Current path\n",
    "    curr_dir = os.path.dirname(os.path.realpath(\"__file__\")) \n",
    "    \n",
    "    if curr_dir.endswith(end_directory):\n",
    "        os.chdir('..')\n",
    "        return curr_dir\n",
    "    \n",
    "    return f'Current working directory: {curr_dir}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Current working directory: /home/campos/projects/ml/federated-Learning-for-text-generation'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_work_directory(end_directory='notebooks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Federated Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Simulation of Federation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate the federation of clients, the module will be used [tff.simulation](https://www.tensorflow.org/federated/api_docs/python/tff/simulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Federation Design**\n",
    "- 715 users  where each example corresponds to a contiguous set of lines spoken by the character in a given play.\n",
    "- For each client, we split the data into a set of training lines(the first 80% of lines for the role), and test lines (the last 20%, rounded up to at least one line).\n",
    "\n",
    "<!-- onde cada exemplo corresponde a um conjunto contíguo de falas faladas pelo personagem em uma determinada peça. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = tff.simulation.datasets.shakespeare.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.client_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "715"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data.client_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Undertanding `tff.simulation.datasets`\n",
    "- [Dataset documentation](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/shakespeare/load_data)\n",
    "- The data and the distributions use the [LEAF: A Benchmark for Federated Settings](https://leaf.cmu.edu/).\n",
    "- The LEAF get the data from [Gutenberg Project](http://www.gutenberg.org/files/100/old/1994-01-100.zip)\n",
    "\n",
    "### Distribute Data Between Users\n",
    "The `tff.simulation.datasets` package provides a variety of datasets that are divided into \"clients\", where each client corresponds to a dataset on a specific device that can participate in federated learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Split Plays by Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting .txt data between users\n",
      "Discarded 5730 lines\n"
     ]
    }
   ],
   "source": [
    "!python src/preprocess_shakespeare.py data/raw/100.txt data/federation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "users_and_plays = open('data/federation/users_and_plays.json', mode='r')\n",
    "json_users_and_plays = json.load(users_and_plays)\n",
    "# pprint.pprint(json_users_and_plays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Client ID\n",
    "The client keys consist of the name of the play joined with the name of the character, so for example `MUCH_ADO_ABOUT_NOTHING_OTHELLO` corresponds to the lines for the character **Othello** in the play **Much Ado About Nothing**. \n",
    "\n",
    "<!-- The IDs of clients consistem no nome da peça junto com o nome do personagem, então por exemplo `MUCH_ADO_ABOUT_NOTHING_OTHELLO` corresponde às falas do personagem Otelo na peça \"Muito Barulho por Nada\". -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data to Tensors\n",
    "The datasets provided by `shakespeare.load_data()` consist of a sequence of string `Tensors`, one for each line spoken by a particular character in a Shakespeare play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow_federated.python.simulation.datasets.client_data.PreprocessClientData"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample\n",
    "\n",
    "Here the play is \"The Tragedy of King Lear\" and the character is \"King\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.ParallelMapDataset'>\n",
      "tf.Tensor(b'', shape=(), dtype=string)\n",
      "tf.Tensor(b'What?', shape=(), dtype=string)\n",
      "tf.Tensor(b'Peace!', shape=(), dtype=string)\n",
      "tf.Tensor(b'[Reads]', shape=(), dtype=string)\n",
      "tf.Tensor(b'Hence, sirs, away.', shape=(), dtype=string)\n",
      "tf.Tensor(b'I was, fair madam.', shape=(), dtype=string)\n",
      "tf.Tensor(b'That can never be.', shape=(), dtype=string)\n",
      "tf.Tensor(b'Upon mine honour, no.', shape=(), dtype=string)\n",
      "tf.Tensor(b\"'that shallow vassal,'\", shape=(), dtype=string)\n",
      "tf.Tensor(b'How fares your Majesty?', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "raw_example_dataset = train_data.create_tf_dataset_for_client('THE_TRAGEDY_OF_KING_LEAR_KING')\n",
    "print(type(raw_example_dataset))\n",
    "\n",
    "for x in raw_example_dataset.take(10):\n",
    "    print(x['snippets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Prepare Dataset for Training RNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the vocab lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A fixed vocabularly of ASCII chars that occur in the works of Shakespeare and Dickens:\n",
    "vocab = list('dhlptx@DHLPTX $(,048cgkoswCGKOSW[_#\\'/37;?bfjnrvzBFJNRVZ\"&*.26:\\naeimquyAEIMQUY]!%)-159\\r')\n",
    "\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input pre-processing parameters\n",
    "SEQ_LENGTH = 100\n",
    "BATCH_SIZE = 8\n",
    "BUFFER_SIZE = 100  # For dataset shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a lookup table (precalculated) to map string chars to indexes, using the vocab loaded above:\n",
    "<!-- \n",
    "- lookup table é uma tabela hash onde a key enderaça diretamente o value.\n",
    "- são tabelas pré-calculadas\n",
    "- economiza tempo de processamento -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.ops.lookup_ops.StaticHashTable"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = tf.constant(list(range(len(vocab))),\n",
    "                     dtype=tf.int64)\n",
    "\n",
    "table = tf\\\n",
    "    .lookup\\\n",
    "    .StaticHashTable(tf\\\n",
    "                     .lookup\\\n",
    "                     .KeyValueTensorInitializer(keys=vocab, values=values), default_value=0)\n",
    "\n",
    "type(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://www.tensorflow.org/federated/tutorials/federated_learning_for_text_generation\n",
    "\n",
    "def to_ids(x):\n",
    "    s = tf.reshape(x['snippets'], shape=[1])\n",
    "    chars = tf.strings.bytes_split(s).values\n",
    "    return table.lookup(chars)\n",
    "  \n",
    "def split_input_target(chunk):\n",
    "    input_text = tf.map_fn(lambda x: x[:-1], chunk)\n",
    "    target_text = tf.map_fn(lambda x: x[1:], chunk)\n",
    "    return (input_text, target_text)\n",
    "\n",
    "def preprocess(dataset):\n",
    "    return (\n",
    "      # Map ASCII chars to int64 indexes using the vocab\n",
    "      dataset.map(to_ids)\n",
    "      # Split into individual chars\n",
    "      .unbatch()\n",
    "      # Form example sequences of SEQ_LENGTH +1\n",
    "      .batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
    "      # Shuffle and form minibatches\n",
    "      .shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "      # And finally split into (input, target) tuples,\n",
    "      # each of length SEQ_LENGTH.\n",
    "      .map(split_input_target)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can preprocess our raw_example_dataset, and check the types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TensorSpec(shape=(8, 100), dtype=tf.int64, name=None), TensorSpec(shape=(8, 100), dtype=tf.int64, name=None))\n"
     ]
    }
   ],
   "source": [
    "example_dataset = preprocess(raw_example_dataset)\n",
    "print(example_dataset.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.MapDataset"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(example_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- [1] Bag-of-Words https://machinelearningmastery.com/gentle-introduction-bag-words-model/\n",
    "- [2] APPLIED FEDERATED LEARNING:IMPROVING GOOGLE KEYBOARD QUERY SUGGESTIONS https://arxiv.org/pdf/1812.02903.pdf\n",
    "- [3] Federated Learning Collaborative https://ai.googleblog.com/2017/04/federated-learning-collaborative.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ivCMBq6x4JD8"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "word_prediction_in_a_mobile_device_federation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
